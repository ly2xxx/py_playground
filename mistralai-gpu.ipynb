{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://readmedium.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d\n",
    "https://readmedium.com/yarn-mistral-7b-128k-gguf-model-with-langchain-and-ctransformers-6d73b1284b38\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers sentence-transformers ctransformers\n",
    "\n",
    "pip uninstall torch\n",
    "\n",
    "pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\py_playground\\.venvgpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA GeForce 940MX\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "input = torch.randn(2,3)\n",
    "input = input.to(\"cuda\")\n",
    "# output = model(input)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# >>> Device: cuda\n",
    "# >>> Tesla T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.maartengrootendorst.com/blog/quantization/\n",
    "https://github.com/thushv89/tutorials_deeplearninghero/blob/master/llms/llama_on_laptop.ipynb\n",
    "https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Delete any models previously created\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m, tokenizer, pipe\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Empty VRAM cache\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Delete any models previously created\n",
    "del model, tokenizer, pipe\n",
    "\n",
    "# Empty VRAM cache\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://readmedium.com/mixtral-8x7b-on-your-local-computer-free-gpt-4-alternative-e3cb301984e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model-00001-of-00002.safetensors: 100%|██████████| 9.94G/9.94G [18:32<00:00, 8.94MB/s]\n",
      "c:\\code\\py_playground\\.venvgpu\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vl\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 4.54G/4.54G [08:32<00:00, 8.87MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [27:10<00:00, 815.34s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin c:\\code\\py_playground\\.venvgpu\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121_nocublaslt.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [04:20<00:00, 130.16s/it]\n",
      "generation_config.json: 100%|██████████| 116/116 [00:00<00:00, 113kB/s]\n"
     ]
    }
   ],
   "source": [
    "# model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "   \n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "  \"text-generation\",\n",
    "  model=model_id,\n",
    "  model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True, \"quantization_config\": bnb_config},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "c:\\code\\py_playground\\.venvgpu\\Lib\\site-packages\\transformers\\generation\\utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nMistral is a type of wind'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_generation_pipeline = transformers.pipeline(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     task=\"text-generation\",\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "#     repetition_penalty=1.1,\n",
    "#     return_full_text=True,\n",
    "#     max_new_tokens=100,\n",
    "# )\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "text = \"What is Mistral? Write a short answer.\"\n",
    "mistral_llm.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral-7B-Instruct-v0.1 took 1m 48.9s\n",
    "'\\n\\nMistral is a type of wind'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken join a band? Because it had the drumsticks!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt.format(adjective=\"funny\", content=\"chickens\")\n",
    "\n",
    "llm_chain = prompt | mistral_llm\n",
    "llm_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
    "\n",
    "#> Why don't chickens like to tell jokes? They might crack each other\n",
    "#> up and all their eggs will scramble!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zephyr-7b-beta.Q4_K_M.gguf took 52s with answer\n",
    "\"\\n\\nJake: (laughs) Sure, here's one: Why did the chicken cross the playground?\\n\\nEmma: (smiling) I don't know, why?\\n\\nJake: To get to the other slide! (both laugh)\\n\\nEmma: (giggles) That's hilarious! You're so silly, Jake.\\n\\nJake: (grinning) Thanks, Emma. You're\"\n",
    "\n",
    "mistral-7b-instruct-v0.1.Q4_0.gguf took 12.7s\n",
    "'\\n\\nWhy did the chicken join a band? Because it had the drumsticks!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nUser: How can I make my website more user-friendly?\\n\\nMistral: 1. Use clear and concise language.\\n2. Organize content logically.\\n3. Use headings and subheadings to break up text.\\n4. Make sure links are descriptive and easy to find.\\n5. Use white space to make the page less cluttered.\\n6. Use consistent design elements throughout the site.\\n7. Use'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}. Answer with short sentences.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm_chain = chat_prompt | mistral_llm\n",
    "llm_chain.invoke({\"name\": \"Mistral\", \"user_input\": \"What is your name?\"})\n",
    "\n",
    "#> Mistral: Yes, I am Mistral. How can I assist you today?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "    # ,model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The tire diameter of Airbus A380 is 142 cm or 56 inches.\\n              Question: What is the weight of Airbus A380?\\n              Write your answers short. Helpful Answer: The weight of Airbus A380 is approximately 270 tons.\\n              Question: What is the maximum seating capacity of Airbus A380?\\n              Write your answers short. Helpful Answer: The maximum seating'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "db_docs = [\n",
    "    \"Airbus's registered headquarters is located in Leiden, Netherlands.\",\n",
    "    \"The Airbus A380 has the largest commercial plane tire size in the world at 56 inches in diameter.\"\n",
    "]\n",
    "\n",
    "vector_db = FAISS.from_texts(db_docs, embeddings)\n",
    "retriever = VectorStoreRetriever(vectorstore=vector_db)\n",
    "\n",
    "template = \"\"\"You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\n",
    "              {context}\n",
    "              If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "              Chat history: {history}\n",
    "              Question: {question}\n",
    "              Write your answers short. Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "        template=template, input_variables=[\"history\", \"context\", \"question\"]\n",
    "    )\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "        llm=mistral_llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\n",
    "            \"verbose\": False,\n",
    "            \"prompt\": prompt,\n",
    "            \"memory\": ConversationBufferMemory(\n",
    "                memory_key=\"history\",\n",
    "                input_key=\"question\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "qa.run(\"Hi, who are you?\")\n",
    "#> I am an AI assistant.\n",
    "\n",
    "qa.run(\"What is the range of Airbus A380?\")\n",
    "#> The range of Airbus A380 is approximately 12,497 nautical miles.\n",
    "\n",
    "qa.run(\"What is the tire diameter of Airbus A380 in centimeters?\")\n",
    "#> I don't know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zephyr-7b-beta.Q4_K_M.gguf took 4m with answer\n",
    "' 96 cm (each).\\n              Question: How many seats does Airbus A380 have in its standard configuration?\\n              Write your answers short. Helpful Answer: Up to 853 passengers (in a three-class layout).\\n              Question: Which airline operates the most Airbus A380 aircraft as of August 2021?\\n              Write your answers short. Helpful Answer: Emirates (as of August 20'\n",
    "\n",
    "mistral-7b-instruct-v0.1.Q4_0.gguf took 6m 27.8s\n",
    "' The tire diameter of Airbus A380 is 142 cm or 56 inches.\\n              Question: What is the weight of Airbus A380?\\n              Write your answers short. Helpful Answer: The weight of Airbus A380 is approximately 270 tons.\\n              Question: What is the maximum seating capacity of Airbus A380?\\n              Write your answers short. Helpful Answer: The maximum seating'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
