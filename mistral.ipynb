{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.36.2\n",
      "Uninstalling transformers-4.36.2:\n",
      "  Would remove:\n",
      "    c:\\python39\\lib\\site-packages\\transformers-4.36.2.dist-info\\*\n",
      "    c:\\python39\\lib\\site-packages\\transformers\\*\n",
      "    c:\\python39\\scripts\\transformers-cli.exe\n",
      "Proceed (Y/n)? \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.2+cpu\n",
      "Is CUDA enabled? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\",torch.__version__)\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146\n",
    "https://medium.com/django-unleashed/how-to-summarize-articles-with-streamlit-and-langchain-with-mistral-7b-on-cpu-8eb6e344a636\n",
    "https://github.com/mlabonne/llm-course\n",
    "https://github.com/simonw/llm-mistral\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 82\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Check GPU compatibility with bfloat16\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#https://www.educative.io/answers/how-to-resolve-torch-not-compiled-with-cuda-enabled\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m#https://forums.developer.nvidia.com/t/is-my-gpu-geforce-940mx-supported-by-cuda/51805/4\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;129;01mand\u001b[39;00m use_4bit:\n\u001b[1;32m---> 82\u001b[0m     major, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m major \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "File \u001b[1;32mc:\\code\\py_playground\\.venvcpu\\Lib\\site-packages\\torch\\cuda\\__init__.py:435\u001b[0m, in \u001b[0;36mget_device_capability\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the cuda capability of a device.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 435\u001b[0m     prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[1;32mc:\\code\\py_playground\\.venvcpu\\Lib\\site-packages\\torch\\cuda\\__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    450\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32mc:\\code\\py_playground\\.venvcpu\\Lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U torch datasets transformers tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
    "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "\n",
    "# from transformers import (\n",
    "#   AutoTokenizer, \n",
    "#   AutoModelForCausalLM, \n",
    "#   BitsAndBytesConfig,\n",
    "#   pipeline\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "#https://www.educative.io/answers/how-to-resolve-torch-not-compiled-with-cuda-enabled\n",
    "#https://forums.developer.nvidia.com/t/is-my-gpu-geforce-940mx-supported-by-cuda/51805/4\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# !playwright install \n",
    "# !playwright install-deps \n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [\"https://www.fantasypros.com/2023/11/rival-fantasy-nfl-week-10/\",\n",
    "            \"https://www.fantasypros.com/2023/11/5-stats-to-know-before-setting-your-fantasy-lineup-week-10/\",\n",
    "            \"https://www.fantasypros.com/2023/11/nfl-week-10-sleeper-picks-player-predictions-2023/\",\n",
    "            \"https://www.fantasypros.com/2023/11/nfl-dfs-week-10-stacking-advice-picks-2023-fantasy-football/\",\n",
    "            \"https://www.fantasypros.com/2023/11/players-to-buy-low-sell-high-trade-advice-2023-fantasy-football/\"]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()\n",
    "\n",
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, \n",
    "                                      chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create prompt template\n",
    "prompt_template = \"\"\"\n",
    "### [INST] Instruction: Answer the question based on your fantasy football knowledge. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    " \"\"\"\n",
    "\n",
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)\n",
    "\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Should I start Gibbs next week for fantasy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Obtaining dependency information for streamlit from https://files.pythonhosted.org/packages/d3/96/9251b421d0a1c7d625a82a04bea56b8a9830c785940ec16db454b85c6db7/streamlit-1.29.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached streamlit-1.29.0-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: langchain in .\\.venvcpu\\lib\\site-packages (0.0.353)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Requirement already satisfied: transformers in .\\.venvcpu\\lib\\site-packages (4.36.2)\n",
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "     ---------------------------------------- 0.0/211.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/211.1 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/211.1 kB ? eta -:--:--\n",
      "     ----- ------------------------------- 30.7/211.1 kB 330.3 kB/s eta 0:00:01\n",
      "     --------------------------------- ---- 184.3/211.1 kB 1.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 211.1/211.1 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Obtaining dependency information for altair<6,>=4.0 from https://files.pythonhosted.org/packages/c5/e4/7fcceef127badbb0d644d730d992410e4f3799b295c9964a172f92a469c7/altair-5.2.0-py3-none-any.whl.metadata\n",
      "  Using cached altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Obtaining dependency information for blinker<2,>=1.0.0 from https://files.pythonhosted.org/packages/fa/2a/7f3714cbc6356a0efec525ce7a0613d581072ed6eb53eb7b9754f33db807/blinker-1.7.0-py3-none-any.whl.metadata\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in .\\.venvcpu\\lib\\site-packages (from streamlit) (5.3.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in .\\.venvcpu\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Collecting importlib-metadata<7,>=1.4 (from streamlit)\n",
      "  Obtaining dependency information for importlib-metadata<7,>=1.4 from https://files.pythonhosted.org/packages/59/9b/ecce94952ab5ea74c31dcf9ccf78ccd484eebebef06019bf8cb579ab4519/importlib_metadata-6.11.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in .\\.venvcpu\\lib\\site-packages (from streamlit) (1.26.2)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in .\\.venvcpu\\lib\\site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in .\\.venvcpu\\lib\\site-packages (from streamlit) (2.1.4)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in .\\.venvcpu\\lib\\site-packages (from streamlit) (10.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in .\\.venvcpu\\lib\\site-packages (from streamlit) (4.23.4)\n",
      "Requirement already satisfied: pyarrow>=6.0 in .\\.venvcpu\\lib\\site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in .\\.venvcpu\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in .\\.venvcpu\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Obtaining dependency information for rich<14,>=10.14.0 from https://files.pythonhosted.org/packages/be/be/1520178fa01eabe014b16e72a952b9f900631142ccd03dc36cf93e30c1ce/rich-13.7.0-py3-none-any.whl.metadata\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in .\\.venvcpu\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in .\\.venvcpu\\lib\\site-packages (from streamlit) (4.9.0)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Obtaining dependency information for tzlocal<6,>=1.1 from https://files.pythonhosted.org/packages/97/3f/c4c51c55ff8487f2e6d0e618dba917e3c3ee2caae6cf0fbb59c9b1876f2e/tzlocal-5.2-py3-none-any.whl.metadata\n",
      "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Obtaining dependency information for validators<1,>=0.2 from https://files.pythonhosted.org/packages/3a/0c/785d317eea99c3739821718f118c70537639aa43f96bfa1d83a71f68eaf6/validators-0.22.0-py3-none-any.whl.metadata\n",
      "  Using cached validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Obtaining dependency information for gitpython!=3.1.19,<4,>=3.0.7 from https://files.pythonhosted.org/packages/8d/c4/82b858fb6483dfb5e338123c154d19c043305b01726a67d89532b8f8f01b/GitPython-3.1.40-py3-none-any.whl.metadata\n",
      "  Using cached GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in .\\.venvcpu\\lib\\site-packages (from streamlit) (6.4)\n",
      "Collecting watchdog>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-3.0.0-py3-none-win_amd64.whl (82 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in .\\.venvcpu\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in .\\.venvcpu\\lib\\site-packages (from langchain) (2.0.24)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\.venvcpu\\lib\\site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in .\\.venvcpu\\lib\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in .\\.venvcpu\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in .\\.venvcpu\\lib\\site-packages (from langchain) (0.0.7)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.4 in .\\.venvcpu\\lib\\site-packages (from langchain) (0.1.4)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in .\\.venvcpu\\lib\\site-packages (from langchain) (0.0.75)\n",
      "Requirement already satisfied: pydantic<3,>=1 in .\\.venvcpu\\lib\\site-packages (from langchain) (2.5.3)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/4c/f3/038b302fdfbe3be7da016777069f26ceefe11a681055ea1f7817546508e3/soupsieve-2.5-py3-none-any.whl.metadata\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: filelock in .\\.venvcpu\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in .\\.venvcpu\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\.venvcpu\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in .\\.venvcpu\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in .\\.venvcpu\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\.venvcpu\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting lxml>=3.6.0 (from newspaper3k)\n",
      "  Obtaining dependency information for lxml>=3.6.0 from https://files.pythonhosted.org/packages/67/4c/6ecc99b31dfbbea2d3327f067921a20c70f86348f9f6cb8c1f06b46d99e1/lxml-5.0.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading lxml-5.0.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: nltk>=3.2.1 in .\\.venvcpu\\lib\\site-packages (from newspaper3k) (3.8.1)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Obtaining dependency information for feedparser>=5.2.1 from https://files.pythonhosted.org/packages/7c/d4/8c31aad9cc18f451c49f7f9cfb5799dadffc88177f7917bc90a66459b1d7/feedparser-6.0.11-py3-none-any.whl.metadata\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Obtaining dependency information for tldextract>=2.0.1 from https://files.pythonhosted.org/packages/d0/de/3f37b2568115c7ebeae39508dc1092f04f3dc286f22ef30171baca9c9cf2/tldextract-5.1.1-py3-none-any.whl.metadata\n",
      "  Downloading tldextract-5.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.5/7.4 MB 9.6 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.9/7.4 MB 9.4 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 1.2/7.4 MB 8.4 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 1.6/7.4 MB 8.5 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 2.1/7.4 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 2.3/7.4 MB 8.0 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 2.7/7.4 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 3.1/7.4 MB 8.7 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 3.4/7.4 MB 7.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 3.9/7.4 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.2/7.4 MB 8.4 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 4.8/7.4 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 5.2/7.4 MB 8.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 5.4/7.4 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 5.8/7.4 MB 8.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 6.3/7.4 MB 8.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 6.3/7.4 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 6.9/7.4 MB 7.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.4/7.4 MB 7.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.4/7.4 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 7.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.venvcpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.venvcpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in .\\.venvcpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.venvcpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in .\\.venvcpu\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: jinja2 in .\\.venvcpu\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for jsonschema>=3.0 from https://files.pythonhosted.org/packages/0f/ed/0058234d8dd2b1fc6beeea8eab945191a05e9d391a63202f49fe23327586/jsonschema-4.20.0-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema-4.20.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: colorama in .\\.venvcpu\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in .\\.venvcpu\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in .\\.venvcpu\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: six in .\\.venvcpu\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Obtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\.venvcpu\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7,>=1.4->streamlit)\n",
      "  Obtaining dependency information for zipp>=0.5 from https://files.pythonhosted.org/packages/d9/66/48866fc6b158c81cc2bfecc04c480f105c6040e8b077bc54c634b4a67926/zipp-3.17.0-py3-none-any.whl.metadata\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\.venvcpu\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in .\\.venvcpu\\lib\\site-packages (from langchain-core<0.2,>=0.1.4->langchain) (4.2.0)\n",
      "Requirement already satisfied: joblib in .\\.venvcpu\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\.venvcpu\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in .\\.venvcpu\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in .\\.venvcpu\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in .\\.venvcpu\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venvcpu\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venvcpu\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venvcpu\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venvcpu\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in .\\.venvcpu\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in .\\.venvcpu\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in .\\.venvcpu\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.4->langchain) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Obtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venvcpu\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/ee/07/44bd408781594c4d0a027666ef27fab1e441b109dc3b76b4f836f8fd04fe/jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/b4/11/d121780c173336c9bc3a5b8240ed31f518957cc22f6311c76259cb0fcf32/referencing-0.32.0-py3-none-any.whl.metadata\n",
      "  Downloading referencing-0.32.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/01/ad/fb34144b299263d770ff38f6f9d64fd10ded46f83d7a7b62ab4a7cca10c3/rpds_py-0.16.2-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading rpds_py-0.16.2-cp311-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in .\\.venvcpu\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Using cached streamlit-1.29.0-py2.py3-none-any.whl (8.4 MB)\n",
      "Using cached altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 81.3/81.3 kB ? eta 0:00:00\n",
      "Using cached GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading lxml-5.0.0-cp311-cp311-win_amd64.whl (3.9 MB)\n",
      "   ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.9 MB 11.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.0/3.9 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.2/3.9 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.4/3.9 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.6/3.9 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.8/3.9 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.0/3.9 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 2.3/3.9 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.6/3.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.9/3.9 MB 5.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.3/3.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.6/3.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.9/3.9 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.9/3.9 MB 5.8 MB/s eta 0:00:00\n",
      "Using cached rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.7/97.7 kB 5.5 MB/s eta 0:00:00\n",
      "Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Using cached validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached jsonschema-4.20.0-py3-none-any.whl (84 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.32.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.16.2-cp311-none-win_amd64.whl (195 kB)\n",
      "   ---------------------------------------- 0.0/195.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 195.8/195.8 kB 6.0 MB/s eta 0:00:00\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13568 sha256=84925356f7542d4d9e1ab5950f27cc55f28233523c41445ae8b1853a77ed9557\n",
      "  Stored in directory: c:\\users\\vl\\appdata\\local\\pip\\cache\\wheels\\fc\\ab\\f8\\cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3359 sha256=a0b246129ecc6eee9300262c5b264b94beaf04ad9e3989c39a5a2e90f75877e5\n",
      "  Stored in directory: c:\\users\\vl\\appdata\\local\\pip\\cache\\wheels\\80\\d5\\72\\9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398387 sha256=7ea09921d17128db662247ea483208d09e30a04c2dd9b4db7a57cae169be162e\n",
      "  Stored in directory: c:\\users\\vl\\appdata\\local\\pip\\cache\\wheels\\3a\\a1\\46\\8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=76b1a46d7582853ea9e8405023499601b1c175c9ac4d3b2ee80ac272958c43f3\n",
      "  Stored in directory: c:\\users\\vl\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, zipp, watchdog, validators, tzlocal, toolz, toml, soupsieve, smmap, rpds-py, mdurl, lxml, feedparser, cssselect, blinker, requests-file, referencing, pydeck, markdown-it-py, importlib-metadata, gitdb, beautifulsoup4, tldextract, rich, jsonschema-specifications, gitpython, feedfinder2, newspaper3k, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.2.0 beautifulsoup4-4.12.2 blinker-1.7.0 cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 gitdb-4.0.11 gitpython-3.1.40 importlib-metadata-6.11.0 jieba3k-0.35.1 jsonschema-4.20.0 jsonschema-specifications-2023.12.1 lxml-5.0.0 markdown-it-py-3.0.0 mdurl-0.1.2 newspaper3k-0.2.8 pydeck-0.8.1b0 referencing-0.32.0 requests-file-1.5.1 rich-13.7.0 rpds-py-0.16.2 sgmllib3k-1.0.0 smmap-5.0.1 soupsieve-2.5 streamlit-1.29.0 tinysegmenter-0.3 tldextract-5.1.1 toml-0.10.2 toolz-0.12.0 tzlocal-5.2 validators-0.22.0 watchdog-3.0.0 zipp-3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit langchain beautifulsoup4 transformers transformers newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain.chains import MapReduceDocumentsChain, LLMChain, ReduceDocumentsChain, StuffDocumentsChain\n",
    "from langchain.document_loaders import NewsURLLoader\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def summarize_article(article_url):\n",
    "    # Load article\n",
    "    loader = NewsURLLoader([article_url])\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Load LLM\n",
    "    config = {'max_new_tokens': 4096, 'temperature': 0.7, 'context_length': 4096}\n",
    "    llm = CTransformers(model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "                        model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "                        config=config,\n",
    "                        threads=os.cpu_count())\n",
    "    \n",
    "    # Map template and chain\n",
    "    map_template = \"\"\"<s>[INST] The following is a part of an article:\n",
    "    {docs}\n",
    "    Based on this, please identify the main points. \n",
    "    Answer:  [/INST] </s>\"\"\"\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "    \n",
    "    # Reduce template and chain\n",
    "    reduce_template = \"\"\"<s>[INST] The following is set of summaries from the article:\n",
    "    {doc_summaries}\n",
    "    Take these and distill it into a final, consolidated summary of the main points. \n",
    "    Construct it as a well organized summary of the main points and should be between 3 and 5 paragraphs.\n",
    "    Answer:  [/INST] </s>\"\"\"\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "    \n",
    "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Combines and iteratively reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=4000,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Combining documents by mapping a chain over them, then combining results\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain,\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"docs\",\n",
    "        # Return the results of the map steps in the output\n",
    "        return_intermediate_steps=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=4000, chunk_overlap=0\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    \n",
    "\n",
    "    # Run the chain\n",
    "    start_time = time.time()\n",
    "    result = map_reduce_chain.__call__(split_docs, return_only_outputs=True)\n",
    "    time_taken = time.time() - start_time\n",
    "    return result['output_text'], time_taken"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
